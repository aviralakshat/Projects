{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Problem using BERT model"
      ],
      "metadata": {
        "id": "xgVhKWSJOuCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnPpzWMlNbx4"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pandas nltk #installing necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet', 'punkt_tab'])\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocessed data\n",
        "print(\"Preprocessing text...\")\n",
        "data['processed_text'] = (data['title'].fillna('') + ' ' + data['description'].fillna('')).apply(preprocess_text)\n",
        "\n",
        "\n",
        "X = data['processed_text'].values\n",
        "y = data['SOC2'].values\n",
        "\n",
        "# Data getting splitted\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# this will take time\n",
        "# for each epoch there will be 32 points batch so like for 1000 there will be 32 batches\n",
        "# this will make the code run epoch*batches times\n",
        "\n",
        "MAX_LEN = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Tokenizing data\n",
        "train_encodings = tokenizer(\n",
        "    X_train.tolist(),\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "#test data tokenization\n",
        "test_encodings = tokenizer(\n",
        "    X_test.tolist(),\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings['input_ids'],\n",
        "    train_encodings['attention_mask'],\n",
        "    torch.tensor(y_train)\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    test_encodings['input_ids'],\n",
        "    test_encodings['attention_mask'],\n",
        "    torch.tensor(y_test)\n",
        ")\n",
        "#dataloaders creation\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ensuring labels are zero-indexed and contiguous BEFORE splitting\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "#splitting data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "\n",
        "unique_labels, counts = np.unique(y, return_counts=True)\n",
        "print(\"Unique labels:\", unique_labels)\n",
        "print(\"Label counts:\", counts)\n",
        "\n",
        "#initializing BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_classes,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# Setting up training parameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 1\n",
        "\n",
        "# Training loop\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {\n",
        "            'input_ids': batch[0],\n",
        "            'attention_mask': batch[1],\n",
        "            'labels': batch[2]\n",
        "        }\n",
        "\n",
        "\n",
        "        inputs['labels'] = torch.clamp(inputs['labels'], 0, num_classes - 1)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nEvaluating model...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    inputs = {\n",
        "        'input_ids': batch[0],\n",
        "        'attention_mask': batch[1],\n",
        "        'labels': batch[2]\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    label_ids = inputs['labels'].cpu().numpy()\n",
        "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "    true_labels.extend(label_ids.flatten())\n",
        "\n",
        "predictions = label_encoder.inverse_transform(predictions)\n",
        "true_labels = label_encoder.inverse_transform(true_labels)\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xd2TFoZaNfYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h-w6u500Ngcz"
      }
    }
  ]
}